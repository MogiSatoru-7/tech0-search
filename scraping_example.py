# ===================================================
# scraping_example.py - Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚µãƒ³ãƒ—ãƒ«
# Week 1 / æŒ‡ä»¤2
# ===================================================

import requests
from bs4 import BeautifulSoup

def scrape_page(url: str) -> dict:
    """
    Webãƒšãƒ¼ã‚¸ã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡ºã™ã‚‹ã€‚

    Args:
        url: å¯¾è±¡URL

    Returns:
        æŠ½å‡ºçµæœã®è¾æ›¸
    """
    response = requests.get(url, timeout=10)
    response.raise_for_status()

    soup = BeautifulSoup(response.text, "html.parser")

    # ã‚¿ã‚¤ãƒˆãƒ«
    title_tag = soup.find("title")
    title = title_tag.get_text().strip() if title_tag else "No Title"

    # meta description
    meta = soup.find("meta", attrs={"name": "description"})
    description = meta.get("content", "") if meta else ""

    # æ®µè½ãƒ†ã‚­ã‚¹ãƒˆ
    paragraphs = soup.find_all("p")
    text = "\n".join(p.get_text().strip() for p in paragraphs)

    # ãƒªãƒ³ã‚¯ä¸€è¦§
    links = [
        a["href"] for a in soup.find_all("a", href=True)
        if a["href"].startswith("http")
    ][:10]

    return {
        "url": url,
        "title": title,
        "description": description,
        "text": text,
        "links": links,
    }


# â”€â”€â”€ å®Ÿè¡Œä¾‹ â”€â”€â”€
if __name__ == "__main__":
    result = scrape_page("https://example.com")

    print(f"ğŸ“„ ã‚¿ã‚¤ãƒˆãƒ«: {result['title']}")
    print(f"ğŸ“ èª¬æ˜: {result['description'][:100]}...")
    print(f"ğŸ”— ãƒªãƒ³ã‚¯æ•°: {len(result['links'])}ä»¶")