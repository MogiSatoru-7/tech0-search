# ===================================================
# app.py - Tech0 Search v0.2
# Week 3 / æŒ‡ä»¤5ãƒ»6
# æ–°æ©Ÿèƒ½: ã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼ ãƒ» å…¨æ–‡æ¤œç´¢ ãƒ» ä¸€æ‹¬ã‚¯ãƒ­ãƒ¼ãƒ«
# ===================================================
import streamlit as st
import json
from pathlib import Path
from datetime import date
from search_fulltext import search_fulltext
from crawler import crawl_url

st.set_page_config(page_title="Tech0 Search v0.2", page_icon="ğŸ”", layout="wide")

DATA_PATH = Path("data/pages.json")

#å…±é€šï¼šIDæ¡ç•ªé–¢æ•°ã‚’è¿½åŠ 
def next_id(pages: list) -> int:
    return (max([p.get("id", 0) for p in pages]) + 1) if pages else 1

def has_url(pages: list, url: str) -> bool:
    return any(p.get("url") == url for p in pages)


@st.cache_data
def load_pages() -> list:
    if DATA_PATH.exists():
        with open(DATA_PATH, "r", encoding="utf-8") as f:
            return json.load(f)
    return []


def save_pages(pages: list):
    DATA_PATH.parent.mkdir(exist_ok=True)
    with open(DATA_PATH, "w", encoding="utf-8") as f:
        json.dump(pages, f, ensure_ascii=False, indent=2)


st.title("ğŸ” Tech0 Search v0.2")
st.caption("PROJECT ZERO â”€ ç¤¾å†…ãƒŠãƒ¬ãƒƒã‚¸æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã€å…¨æ–‡æ¤œç´¢å¯¾å¿œã€‘")

tab_search, tab_crawl, tab_register, tab_list = st.tabs(
    ["ğŸ” æ¤œç´¢", "ğŸ¤– ã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼", "ğŸ“ æ‰‹å‹•ç™»éŒ²", "ğŸ“‹ ä¸€è¦§"]
)

pages = load_pages()


# â”â”â” æ¤œç´¢ã‚¿ãƒ– â”â”â”
with tab_search:
    query = st.text_input("ğŸ” ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›", placeholder="ä¾‹: DX, IoT, è£½é€ æ¥­")

    if query:
        results = search_fulltext(query, pages)

        st.markdown(f"**ğŸ“Š æ¤œç´¢çµæœ: {len(results)}ä»¶**ï¼ˆãƒãƒƒãƒæ•°é †ï¼‰")
        st.divider()

        for page in results:
            with st.container():
                c_title, c_score = st.columns([4, 1])
                c_title.markdown(f"### ğŸ“„ {page['title']}")
                c_score.metric("ãƒãƒƒãƒ", f"{page['match_count']}å›")

                st.markdown(f"*{page.get('preview', page.get('description', ''))}*")

                if page.get("keywords"):
                    st.markdown("ğŸ·ï¸ " + " ".join(f"`{kw}`" for kw in page["keywords"][:5]))

                c1, c2, c3 = st.columns(3)
                c1.caption(f"ğŸ‘¤ {page.get('author', 'ä¸æ˜')}")
                c2.caption(f"ğŸ“Š {page.get('word_count', 0)}èª")
                c3.caption(f"ğŸ“… {page.get('created_at', '')[:10]}")

                st.markdown(f"ğŸ”— [{page['url']}]({page['url']})")
                st.divider()

        if not results:
            st.info("è©²å½“ã™ã‚‹ãƒšãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")


# â”â”â” ã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼ã‚¿ãƒ– â”â”â”
with tab_crawl:
    st.subheader("ğŸ¤– è‡ªå‹•ã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼")
    st.info("URLã‚’å…¥åŠ›ã™ã‚‹ã¨ã€è‡ªå‹•ã§ãƒšãƒ¼ã‚¸æƒ…å ±ã‚’å–å¾—ã—ã¾ã™")

    # â”€â”€ å˜ä½“ã‚¯ãƒ­ãƒ¼ãƒ« â”€â”€
    target_url = st.text_input("ã‚¯ãƒ­ãƒ¼ãƒ«å¯¾è±¡URL", placeholder="https://example.com")
    
    #ã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼ã‚¿ãƒ–ï¼šSSLã‚¹ã‚­ãƒƒãƒ—è¿½åŠ ï¼†IDæ¡ç•ªä¿®æ­£
    skip_ssl = st.checkbox("SSLæ¤œè¨¼ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆæ¤œè¨¼ç”¨ï¼‰", value=False)

    #session_stateå¯¾å¿œ
    if "crawl_result" not in st.session_state:
        st.session_state.crawl_result = None

    if st.button("ğŸ¤– ã‚¯ãƒ­ãƒ¼ãƒ«å®Ÿè¡Œ", type="primary", key="btn_crawl_single"):
        if target_url:
            with st.spinner("ã‚¯ãƒ­ãƒ¼ãƒ«ä¸­..."):
                st.session_state.crawl_result = crawl_url(target_url, verify_ssl=not skip_ssl)
        else:
            st.warning("URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")

    result = st.session_state.crawl_result

    if result and result.get("crawl_status") == "success":
        st.success("âœ… ã‚¯ãƒ­ãƒ¼ãƒ«æˆåŠŸï¼")

        c1, c2 = st.columns(2)
        c1.metric("ğŸ“„ ã‚¿ã‚¤ãƒˆãƒ«", (result["title"][:30] + "...") if len(result["title"]) > 30 else result["title"])
        c1.metric("ğŸ“Š æ–‡å­—æ•°", f"{result.get('word_count', 0)}èª")
        c2.metric("ğŸ”— ãƒªãƒ³ã‚¯æ•°", f"{len(result.get('links', []))}ä»¶")
        c2.metric("ğŸ·ï¸ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", f"{len(result.get('keywords', []))}å€‹")

        st.markdown("**ğŸ“– æœ¬æ–‡ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼:**")
        ft = result.get("full_text", "")
        st.write(ft[:500] + ("..." if len(ft) > 500 else ""))

        if st.button("ğŸ’¾ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«ç™»éŒ²", key="btn_register_single"):
            if has_url(pages, result["url"]):
                st.warning("åŒã˜URLãŒæ—¢ã«ç™»éŒ²ã•ã‚Œã¦ã„ã¾ã™ï¼ˆã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸï¼‰")
            else:
                r = result.copy()
                r["id"] = next_id(pages)
                r["author"] = "ã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼"
                r["category"] = "è‡ªå‹•å–å¾—"
                r["created_at"] = r["crawled_at"][:10]
                pages.append(r)
                save_pages(pages)
                st.success(f"âœ… ã€Œ{r['title']}ã€ã‚’ç™»éŒ²ã—ã¾ã—ãŸï¼")
                st.cache_data.clear()
                st.session_state.crawl_result = None
                st.rerun()

    elif result and result.get("crawl_status") != "success":
        st.error(f"âŒ ã‚¯ãƒ­ãƒ¼ãƒ«å¤±æ•—: {result.get('error', 'Unknown')}")

    #session_stateå‰
    # if st.button("ğŸ¤– ã‚¯ãƒ­ãƒ¼ãƒ«å®Ÿè¡Œ", type="primary", key="btn_crawl_single"):
    #     if target_url:
    #         with st.spinner("ã‚¯ãƒ­ãƒ¼ãƒ«ä¸­..."):
    #             # result = crawl_url(target_url)
    #             result = crawl_url(target_url, verify_ssl=not skip_ssl)

    #         if result.get("crawl_status") == "success":
    #             st.success("âœ… ã‚¯ãƒ­ãƒ¼ãƒ«æˆåŠŸï¼")

    #             c1, c2 = st.columns(2)
    #             c1.metric("ğŸ“„ ã‚¿ã‚¤ãƒˆãƒ«", result["title"][:30] + "...")
    #             c1.metric("ğŸ“Š æ–‡å­—æ•°", f"{result['word_count']}èª")
    #             c2.metric("ğŸ”— ãƒªãƒ³ã‚¯æ•°", f"{len(result.get('links', []))}ä»¶")
    #             c2.metric("ğŸ·ï¸ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", f"{len(result.get('keywords', []))}å€‹")

    #             st.markdown("**ğŸ“– æœ¬æ–‡ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼:**")
    #             preview = result.get("full_text", "")[:500]
    #             st.write(preview + ("..." if len(result.get("full_text", "")) > 500 else ""))

    #             if st.button("ğŸ’¾ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«ç™»éŒ²"):
    #                 if has_url(pages, result["url"]):
    #                     st.warning("åŒã˜URLãŒæ—¢ã«ç™»éŒ²ã•ã‚Œã¦ã„ã¾ã™ï¼ˆã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸï¼‰")  #ifè¿½åŠ 
    #                 else:
    #                     result["id"] = next_id(pages)
    #                     # result["id"] = len(pages) + 1
    #                     result["author"] = "ã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼"
    #                     result["category"] = "è‡ªå‹•å–å¾—"
    #                     result["created_at"] = result["crawled_at"][:10]
    #                     pages.append(result)
    #                     save_pages(pages)
    #                     st.success(f"âœ… ã€Œ{result['title']}ã€ã‚’ç™»éŒ²ã—ã¾ã—ãŸï¼")
    #                     st.cache_data.clear()
    #                     st.rerun()
    #         else:
    #             st.error(f"âŒ ã‚¯ãƒ­ãƒ¼ãƒ«å¤±æ•—: {result.get('error', 'Unknown')}")

    # â”€â”€ ä¸€æ‹¬ã‚¯ãƒ­ãƒ¼ãƒ« â”€â”€
    st.divider()
    st.subheader("ğŸ“‹ ä¸€æ‹¬ã‚¯ãƒ­ãƒ¼ãƒ«")

    urls_text = st.text_area("URLãƒªã‚¹ãƒˆï¼ˆ1è¡Œã«1URLï¼‰", height=150,
                             placeholder="https://example1.com\\nhttps://example2.com")

    if st.button("ğŸš€ ä¸€æ‹¬ã‚¯ãƒ­ãƒ¼ãƒ«å®Ÿè¡Œ"):
        urls = [u.strip() for u in urls_text.splitlines() if u.strip()]
        if urls:
            bar = st.progress(0)
            ok = 0
            for i, u in enumerate(urls):
                if has_url(pages, u):
                    bar.progress((i + 1) / len(urls))
                    continue

                r = crawl_url(u, verify_ssl=not skip_ssl)
                if r.get("crawl_status") == "success":
                    r["id"] = next_id(pages)
                    r["author"] = "ã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼"
                    r["category"] = "è‡ªå‹•å–å¾—"
                    r["created_at"] = r["crawled_at"][:10]
                    pages.append(r)
                    ok += 1
                bar.progress((i + 1) / len(urls))

            # for i, u in enumerate(urls):
            #     r = crawl_url(u)
            #     if r.get("crawl_status") == "success":
            #         r["id"] = len(pages) + 1
            #         r["author"] = "ã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼"
            #         r["category"] = "è‡ªå‹•å–å¾—"
            #         r["created_at"] = r["crawled_at"][:10]
            #         pages.append(r)
            #         ok += 1
            #     bar.progress((i + 1) / len(urls))

            save_pages(pages)
            st.cache_data.clear()
            st.success(f"âœ… {ok}/{len(urls)}ä»¶ ã‚¯ãƒ­ãƒ¼ãƒ«å®Œäº†ï¼")


# â”â”â” æ‰‹å‹•ç™»éŒ²ã‚¿ãƒ– â”â”â”
with tab_register:
    st.subheader("æ–°è¦ãƒšãƒ¼ã‚¸ç™»éŒ²ï¼ˆæ‰‹å‹•ï¼‰")
    with st.form("reg"):
        url   = st.text_input("URL *")
        title = st.text_input("ã‚¿ã‚¤ãƒˆãƒ« *")
        desc  = st.text_area("èª¬æ˜ *")
        kws   = st.text_input("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šï¼‰")
        auth  = st.text_input("ä½œæˆè€…")
        cat   = st.selectbox("ã‚«ãƒ†ã‚´ãƒª", ["è‡ªå·±ç´¹ä»‹", "ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆ", "äº‹ä¾‹", "ãã®ä»–"])
        go    = st.form_submit_button("ğŸ“ ç™»éŒ²", type="primary")

    if go and url and title and desc:
        pages.append({
            "id": next_id(pages),
            "url": url,
            "title": title,
            "description": desc,
            "full_text": desc,
            "word_count": len(desc.split()),
            "keywords": [k.strip() for k in kws.split(",") if k.strip()],
            "author": auth,
            "created_at": str(date.today()),
            "category": cat,
            "crawl_status": "manual",
            "crawled_at": None,
        })

        # pages.append({
        #     "id": len(pages) + 1, "url": url, "title": title,
        #     "description": desc,
        #     "keywords": [k.strip() for k in kws.split(",") if k.strip()],
        #     "author": auth, "created_at": str(date.today()), "category": cat,
        # })
        save_pages(pages)
        st.success(f"âœ… ã€Œ{title}ã€ã‚’ç™»éŒ²ã—ã¾ã—ãŸï¼")
        st.cache_data.clear()


# â”â”â” ä¸€è¦§ã‚¿ãƒ– â”â”â”
with tab_list:
    st.subheader(f"ğŸ“‹ ç™»éŒ²æ¸ˆã¿ãƒšãƒ¼ã‚¸ï¼ˆ{len(pages)}ä»¶ï¼‰")
    for p in pages:
        with st.expander(f"ğŸ“„ {p['title']}"):
            st.write(f"**URL:** {p['url']}")
            st.write(f"**æ–‡å­—æ•°:** {p.get('word_count', 0)}èª")
            st.write(f"**ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹:** {p.get('crawl_status', 'æ‰‹å‹•ç™»éŒ²')}")


st.divider()
st.caption("Â© 2025 PROJECT ZERO â”€ Tech0 Search v0.2")