# ===================================================
# crawler.py - Webã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆLevel 2ï¼‰
# Week 3 / æŒ‡ä»¤5
# ===================================================
import requests
from bs4 import BeautifulSoup
from datetime import datetime
from typing import Optional
import re

def fetch_page(url: str, timeout: int = 10, verify_ssl: bool = True) -> Optional[str]:
# def fetch_page(url: str, timeout: int = 10) -> Optional[str]:
    """
    æŒ‡å®šURLã®HTMLã‚’å–å¾—ã™ã‚‹ã€‚

    Args:
        url:     å–å¾—å¯¾è±¡URL
        timeout: ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆç§’æ•°

    Returns:
        HTMLæ–‡å­—åˆ—ã€‚å¤±æ•—æ™‚ã¯ None
    """
    try:
        headers = {"User-Agent": "Tech0SearchBot/1.0 (Educational Purpose)"}
        resp = requests.get(url, headers=headers, timeout=timeout, verify=verify_ssl)
        # resp = requests.get(url, headers=headers, timeout=timeout)
        resp.raise_for_status()
        resp.encoding = resp.apparent_encoding
        return resp.text
    except requests.RequestException as e:
        print(f"âŒ å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return None


def parse_html(html: str, url: str) -> dict:
    """
    HTMLã‚’è§£æã—ã¦ãƒšãƒ¼ã‚¸æƒ…å ±ã‚’æŠ½å‡ºã™ã‚‹ã€‚

    Args:
        html: HTMLæ–‡å­—åˆ—
        url:  å…ƒURL

    Returns:
        æŠ½å‡ºã—ãŸæƒ…å ±ã®è¾æ›¸
    """
    soup = BeautifulSoup(html, "html.parser")

    # ä¸è¦ã‚¿ã‚°ã‚’é™¤å»
    for tag in soup(["script", "style", "nav", "footer", "header"]):
        tag.decompose()

    # â”€â”€ ã‚¿ã‚¤ãƒˆãƒ« â”€â”€
    title = "No Title"
    if soup.find("title"):
        title = soup.find("title").get_text().strip()
    elif soup.find("h1"):
        title = soup.find("h1").get_text().strip()

    # â”€â”€ meta description â”€â”€
    description = ""
    meta = soup.find("meta", attrs={"name": "description"})
    if meta and meta.get("content"):
        description = meta["content"][:200]

    # â”€â”€ meta keywords â”€â”€
    keywords = []
    meta_kw = soup.find("meta", attrs={"name": "keywords"})
    if meta_kw and meta_kw.get("content"):
        keywords = [kw.strip() for kw in meta_kw["content"].split(",")][:10]

    # â”€â”€ æœ¬æ–‡ãƒ†ã‚­ã‚¹ãƒˆ â”€â”€
    elems = soup.find_all(["p", "h1", "h2", "h3", "h4", "h5", "h6", "li", "td"])
    full_text = " ".join(e.get_text().strip() for e in elems)
    full_text = re.sub(r"\s+", " ", full_text).strip()
    # full_text = re.sub(r"\\s+", " ", full_text).strip()

    # â”€â”€ ãƒªãƒ³ã‚¯ â”€â”€
    links = [
        a["href"]
        for a in soup.find_all("a", href=True)
        if a["href"].startswith("http")
    ][:20]

    return {
        "url": url,
        "title": title,
        "description": description,
        "keywords": keywords,
        "full_text": full_text,
        "links": links,
        "word_count": len(full_text.split()),
        "crawled_at": datetime.now().isoformat(),
        "crawl_status": "success",
    }


# def crawl_url(url: str) -> dict:
def crawl_url(url: str, verify_ssl: bool = True) -> dict:
    """
    URLã‚’ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã¦æƒ…å ±ã‚’è¿”ã™ï¼ˆfetch â†’ parse ã®ãƒ¯ãƒ³ã‚¹ãƒˆãƒƒãƒ—ï¼‰ã€‚

    Args:
        url: ã‚¯ãƒ­ãƒ¼ãƒ«å¯¾è±¡URL

    Returns:
        ãƒšãƒ¼ã‚¸æƒ…å ±ã®è¾æ›¸ï¼ˆå¤±æ•—æ™‚ã‚‚ crawl_status ã§åˆ¤åˆ¥å¯èƒ½ï¼‰
    """
    # html = fetch_page(url)
    html = fetch_page(url, verify_ssl=verify_ssl)
    if not html:
        return {
            "url": url,
            "crawl_status": "failed",
            "crawled_at": datetime.now().isoformat(),
            "error": "Failed to fetch page",
        }
    try:
        return parse_html(html, url)
    except Exception as e:
        return {
            "url": url,
            "crawl_status": "error",
            "crawled_at": datetime.now().isoformat(),
            "error": str(e),
        }



#ä»¥ä¸‹ã€Jsonã«è¿½åŠ ã™ã‚‹ã‚³ãƒ¼ãƒ‰#########
# def append_to_pages_json(result: dict, data_path: str = "data/pages.json") -> bool:
#     import json
#     from pathlib import Path

#     path = Path(data_path)
#     path.parent.mkdir(parents=True, exist_ok=True)

#     pages = json.loads(path.read_text(encoding="utf-8")) if path.exists() else []

#     if result.get("crawl_status") != "success":
#         return False

#     result = result.copy()
#     result["id"] = (max([p.get("id", 0) for p in pages]) + 1) if pages else 1
#     result.setdefault("author", "crawler")
#     result.setdefault("category", "è‡ªå‹•å–å¾—")
#     result.setdefault("created_at", result["crawled_at"][:10])

#     pages.append(result)
#     path.write_text(json.dumps(pages, ensure_ascii=False, indent=2), encoding="utf-8")
#     return True


# â”€â”€â”€ ãƒ†ã‚¹ãƒˆ â”€â”€â”€
if __name__ == "__main__":
    # result = crawl_url("https://example.com")
    url = "https://shun-y-12-tec0-portfolio.netlify.app/"    #è‡ªå·±ç´¹ä»‹LPã‚’è²¼ã‚‹
    result = crawl_url(url, verify_ssl=False)  #èŒ‚æœ¨ã¯ã‚¨ãƒ©ãƒ¼ãŒå‡ºã‚‹ã®ã§ã“ã†ã—ã¦ã„ã‚‹ãŒã€æœ¬æ¥ã¯True
    if result.get("crawl_status") == "success":
        print("âœ… ã‚¯ãƒ­ãƒ¼ãƒ«æˆåŠŸ!")
        print(f"ğŸ“„ ã‚¿ã‚¤ãƒˆãƒ«: {result['title']}")
        print(f"ğŸ“ èª¬æ˜: {result['description'][:100]}...")
        print(f"ğŸ“Š æ–‡å­—æ•°: {result['word_count']}èª")
        print(f"ğŸ”— ãƒªãƒ³ã‚¯æ•°: {len(result['links'])}ä»¶")
        ok = append_to_pages_json(result)
    else:
        print(f"âŒ ã‚¯ãƒ­ãƒ¼ãƒ«å¤±æ•—: {result.get('error')}")
